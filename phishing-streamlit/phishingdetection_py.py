# -*- coding: utf-8 -*-
"""phishingdetection.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnIoqV6wA0KxRJWHSPQZE4BlKDripy7v
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy scikit-learn matplotlib seaborn streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

url="https://drive.google.com/uc?id=1C4ABE0ZEFDzl1M-3mYmjIcxAGlQCzBxP"
df=pd.read_csv(url)
df.head()

print(df.shape)
print(df.info())
print(df.isnull().sum())
df.describe()

df['class'].value_counts()

sns.countplot(x='class',data=df)
plt.title("Distribution of Safe vs Phishing sites")
plt.show()

plt.figure(figsize=(12,6))
sns.heatmap(df.corr(),cmap='coolwarm')
plt.title('Correlation heatmap')
plt.show()

X=df.drop(columns=['Index','class'])
y=df['class']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100,random_state=42)
rf_model.fit(X_train,y_train)
y_pred = rf_model.predict(X_test)

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Precision:",precision_score(y_test,y_pred))
print("Recall:",recall_score(y_test,y_pred))
print("F1 Score:",f1_score(y_test,y_pred))
print("Confusion matrix:\n",confusion_matrix(y_test,y_pred))

feat_importance=pd.Series(rf_model.feature_importances_,index=X.columns)
feat_importance.nlargest(10).plot(kind='barh')
plt.title("Top 10 important features")
plt.show()

sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix Heatmap")
plt.show()

import joblib
joblib.dump(rf_model,'phishing_rf_model.pkl')

loaded_model = joblib.load('phishing_rf_model.pkl')
print('Model loaded successfully')
y_pred_loaded = loaded_model.predict(X_test)
print("Accuracy (Loaded model):",accuracy_score(y_test,y_pred_loaded))

def predict_site(features):
    features = np.array(features).reshape(1,-1)
    prediction = loaded_model.predict(features)[0]
    return 'Phishing' if prediction ==  1 else 'Safe'

sample = X_test.iloc[0].values
print(f"The site is {predict_site(sample)}")

import streamlit as st
from urllib.parse import urlparse
import re

loaded_model = joblib.load('phishing_rf_model.pkl')
print('Model loaded successfully')

st.title("Phishing Website Detection")
st.write("Enter the URL of the website to predict if it is a phishing site or a safe site.")

url_input = st.text_input("Enter Website URL")

predict_button = st.button("Predict")

def extract_features(url):
    features = {}
    # Features from X.columns excluding 'Index'
    feature_list = [
        'UsingIP', 'LongURL', 'ShortURL', 'Symbol@', 'Redirecting//',
        'PrefixSuffix-', 'SubDomains', 'HTTPS', 'DomainRegLen', 'Favicon',
        'NonStdPort', 'HTTPSDomainURL', 'RequestURL', 'AnchorURL',
        'LinksInScriptTags', 'ServerFormHandler', 'InfoEmail', 'AbnormalURL',
        'WebsiteForwarding', 'StatusBarCust', 'DisableRightClick',
        'UsingPopupWindow', 'IframeRedirection', 'AgeofDomain', 'DNSRecording',
        'WebsiteTraffic', 'PageRank', 'GoogleIndex', 'LinksPointingToPage',
        'StatsReport'
    ]

    try:
        parsed_url = urlparse(url)
        # Example feature extraction (simplified - need to add logic for all features)
        features['UsingIP'] = 1 if re.match(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', parsed_url.netloc) else -1
        features['LongURL'] = 1 if len(url) > 75 else -1 # Example threshold
        features['ShortURL'] = 1 if re.search(r'bit\.ly|goo\.gl|tinyurl\.com', url) else -1
        features['Symbol@'] = 1 if '@' in parsed_url.netloc else -1
        features['Redirecting//'] = 1 if '//' in parsed_url.path else -1
        features['PrefixSuffix-'] = 1 if '-' in parsed_url.netloc else -1
        features['SubDomains'] = 0 if parsed_url.netloc.count('.') <= 1 else (1 if parsed_url.netloc.count('.') <= 3 else -1) # Simplified
        features['HTTPS'] = 1 if parsed_url.scheme == 'https' else -1
        features['DomainRegLen'] = 1 # Placeholder - requires external lookup
        features['Favicon'] = 1 # Placeholder - requires fetching website
        features['NonStdPort'] = 1 # Placeholder - requires checking port
        features['HTTPSDomainURL'] = 1 # Placeholder - requires checking domain in HTTPS
        features['RequestURL'] = 1 # Placeholder - requires fetching website
        features['AnchorURL'] = 1 # Placeholder - requires fetching website
        features['LinksInScriptTags'] = 1 # Placeholder - requires fetching website
        features['ServerFormHandler'] = 1 # Placeholder - requires fetching website
        features['InfoEmail'] = 1 if 'mailto:' in url else -1
        features['AbnormalURL'] = 1 # Placeholder - requires complex analysis
        features['WebsiteForwarding'] = 1 # Placeholder - requires fetching website
        features['StatusBarCust'] = 1 # Placeholder - requires fetching website
        features['DisableRightClick'] = 1 # Placeholder - requires fetching website
        features['UsingPopupWindow'] = 1 # Placeholder - requires fetching website
        features['IframeRedirection'] = 1 # Placeholder - requires fetching website
        features['AgeofDomain'] = 1 # Placeholder - requires external lookup
        features['DNSRecording'] = 1 # Placeholder - requires external lookup
        features['WebsiteTraffic'] = 1 # Placeholder - requires external lookup
        features['PageRank'] = 1 # Placeholder - requires external lookup
        features['GoogleIndex'] = 1 # Placeholder - requires external lookup
        features['LinksPointingToPage'] = 1 # Placeholder - requires external lookup
        features['StatsReport'] = 1 # Placeholder - requires external lookup


    except Exception as e:
        st.error(f"Error extracting features: {e}")
        return None

    # Ensure features are in the correct order and format
    extracted_features = [features.get(feat, 0) for feat in feature_list]

    return extracted_features

if predict_button and url_input:
    features = extract_features(url_input)
    if features:
        # Convert to numpy array for prediction
        features_array = np.array(features).reshape(1, -1)

        # Make prediction
        prediction = loaded_model.predict(features_array)[0]

        # Display result (moved from previous subtask for better flow)
        st.subheader("Prediction Result:")
        if prediction == 1:
            st.success("Phishing site detected")
        else:
            st.info("Safe site")

elif predict_button and not url_input:
    st.warning("Please enter a URL to predict.")

# To run this Streamlit application in Google Colab:

# 1. Save this notebook as a Python file (e.g., phishing_detector.py).
#    Go to File -> Download -> Download .py (Python).

# 2. Open a new code cell and run the following command, replacing 'phishing_detector.py' with the actual filename:
# !streamlit run phishing_detector.py & npx localtunnel --port 8501

# 3. The command will output a public URL (usually ending with localtunnel.me) that you can click to access the Streamlit application in your browser.

# print(X_test.columns)